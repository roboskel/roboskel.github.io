<!--In this paper, we present RoboMAE, a multi-modal
sensor data annotation environment that allows humans to
concentrate on high-level decisions producing full frame-by-frame
annotations. Multi-modal annotation tools focus on interpreting a
scene by annotating data on separate modalities. In this work, we
focus on the cross-linking of the same objectâ€™s recognition across
the different modalities. Our approach is based on exploiting
spatio-temporal co-occurrence to link the different projections
of the same object in the various supported modalities and
on automatically interpolating annotations between explicitly
annotated frames. The backend automations interact with the
visual environment in real time, providing annotators with immediate feedback for their actions. Our approach is demonstrated
and evaluated on a dataset collected for the recognition and
localization of conversing humans, an important task in human-
robot interaction applications. Both the annotation environment
and the conversation dataset are made publicly available.-->
